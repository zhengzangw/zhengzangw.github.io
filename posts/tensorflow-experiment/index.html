<!DOCTYPE html>
<html lang="en-us">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>A toy experiments on Tensorflow 1 | Zangwei</title>

    
    <link rel="stylesheet" href="/scss/main.min.6c4d523b15a1f1714ec1a02eecf7283de3733cb142ca8bf9edd01f9f077cc730.css" integity="sha256-bE1SOxWh8XFOwaAu7PcoPeNzPLFCyov57dAfnwd8xzA=">

    <script type="text/javascript" src="/js/dark.js"></script>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script>
  MathJax = {
    tex: {
      inlineMath: [["$", "$"]],
    },
    displayMath: [
      ["$$", "$$"],
      ["\[\[", "\]\]"],
    ],
    svg: {
      fontCache: "global",
    },
  };

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
></script>

    <link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
    <link rel="manifest" href="/favicon/site.webmanifest">
</head><body class="app auto flex-container">
    <div class="flex-container flex-column"><nav>
    <hr>
    <div class="flex-container flex-row flex-row-full">
        
        <div class="nav-item">
            <a href="/about">[ About ]</a>
        </div>
        
        <div class="nav-item">
            <a href="/pdfs/resume.pdf">[ CV ]</a>
        </div>
        
        <div class="nav-item">
            <a href="/#Publications">[ Publication ]</a>
        </div>
        
        <div class="nav-item">
            <a href="/#Teaching-Assistant">[ Teaching ]</a>
        </div>
        
        <div class="nav-item">
            <a href="/notes">[ Notes ]</a>
        </div>
        
        <div class="nav-item">
            <a href="/friends">[ Friends ]</a>
        </div>
        
        <div class="nav-item btn btn-switch">
            <a>[ <span class="theme-name">Auto</span> ]</a>
        </div>
    </div>
    <hr>
</nav>
<div class="flex-passage flex-row flex-row-full">
            <div class="flex-column-20">
                <div class="return">
                    <a href=".."> RETURN </a>
                </div>
                <nav id="TableOfContents">
  <ul>
    <li><a href="#实验目标">实验目标</a>
      <ul>
        <li><a href="#easy">Easy</a></li>
        <li><a href="#hard">Hard</a></li>
      </ul>
    </li>
    <li><a href="#实验结果">实验结果</a>
      <ul>
        <li><a href="#对照实验性能">对照实验性能</a></li>
        <li><a href="#无激活函数">无激活函数</a></li>
        <li><a href="#隐层使用激活函数">隐层使用激活函数</a></li>
        <li><a href="#使用mse作为优化目标">使用MSE作为优化目标</a></li>
        <li><a href="#使用批量梯度下降">使用批量梯度下降</a></li>
        <li><a href="#隐层宽度由3变4">隐层宽度由3变4</a></li>
        <li><a href="#两隐层设置学习率">两隐层&amp;设置学习率</a></li>
        <li><a href="#adam优化">Adam优化</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#实验目标-1">实验目标</a></li>
    <li><a href="#实验结果-1">实验结果</a>
      <ul>
        <li><a href="#深度学习83基本算法">《深度学习》8.3基本算法</a></li>
        <li><a href="#参数自适应学习率算法">参数自适应学习率算法</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#实验目标-2">实验目标</a>
      <ul>
        <li><a href="#easy-1">EASY</a></li>
        <li><a href="#hard-1">HARD</a></li>
      </ul>
    </li>
    <li><a href="#实验结果-2">实验结果</a>
      <ul>
        <li><a href="#第一项性能">第一项性能</a></li>
        <li><a href="#规范化后">规范化后</a></li>
        <li><a href="#正则化讨论">正则化讨论</a></li>
        <li><a href="#学习率">学习率</a></li>
        <li><a href="#滑动平均">滑动平均</a></li>
        <li><a href="#宽度">宽度</a></li>
        <li><a href="#深度">深度</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#实验目标-3">实验目标</a></li>
    <li><a href="#实验结果-3">实验结果</a>
      <ul>
        <li><a href="#激活函数">激活函数</a></li>
        <li><a href="#dropout">dropout</a></li>
        <li><a href="#径向基网络">径向基网络</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#实验目标-4">实验目标</a>
      <ul>
        <li><a href="#easy-2">Easy</a></li>
        <li><a href="#hard-2">Hard</a></li>
        <li><a href="#实验结果-4">实验结果</a></li>
      </ul>
    </li>
  </ul>
</nav>
            </div>
            <main class="flex-column-80"><h1 id="前言">前言</h1>
<p>《深度学习》终于获得了“过半警告”，不过CNN,RNN都还没学。前面的很多算法都没有实践看过效果，决定在Tensorflow上实现一下。</p>
<p>参考的资料有：《Tensorflow 实现Goolge深度学习框架》 莫烦的网络教程、</p>
<p>这个系列尽可能做的好一点，加深对参数的理解和使用</p>
<h1 id="实验一简单mlp搭建">实验一：简单MLP搭建</h1>
<h2 id="实验目标">实验目标</h2>
<h3 id="easy">Easy</h3>
<ul>
<li>完成单隐层神经网络的程序</li>
<li>观察是否有bias在异或数据集上的效果</li>
<li>观察输出层激活函数在异或数据集上的效果</li>
<li>观察隐层激活函数在异或数据集上的效果</li>
<li>观察小批量梯度下降的效果</li>
<li>观察MSE和cross_entropy损失函数在异或数据集上的效果</li>
<li>设置单隐层宽度</li>
<li>设置多隐层</li>
<li>设置学习率</li>
</ul>
<h3 id="hard">Hard</h3>
<ul>
<li>理解滑动平均模型</li>
<li>调节Adam算法参数，比较其与Gradient Descent的优劣</li>
</ul>
<h2 id="实验结果">实验结果</h2>
<p>数据随机生成，数据量 100
对照实验为：SGD(batch = 8),GD(alpha=0.5),输出层激活函数sigmoid,损失函数cross_entropy
共500轮，每50轮输出一次</p>
<h3 id="对照实验性能">对照实验性能</h3>
<blockquote>
<p>Turn 400, loss is 0.0004411380796227604, MSE is 0.529999315738678<br>
Turn 450, loss is 0.00038424794911406934, MSE is 0.5299994349479675<br>
Turn 500, loss is 0.00033973221434280276, MSE is 0.5299995541572571</p>
</blockquote>
<h3 id="无激活函数">无激活函数</h3>
<p>无法完成该任务</p>
<h3 id="隐层使用激活函数">隐层使用激活函数</h3>
<p>效果竟然没有一开始好了</p>
<blockquote>
<p>Turn 400, loss is 0.003771318821236491, MSE is 0.5296160578727722<br>
Turn 450, loss is 0.0033036477398127317, MSE is 0.5296692848205566<br>
Turn 500, loss is 0.002936921315267682, MSE is 0.5297102332115173</p>
</blockquote>
<h3 id="使用mse作为优化目标">使用MSE作为优化目标</h3>
<blockquote>
<p>Turn 400, loss is 0.1910565048456192, MSE is 0.032664474099874496<br>
Turn 450, loss is 0.180148646235466, MSE is 0.030407795682549477<br>
Turn 500, loss is 0.17121551930904388, MSE is 0.028623349964618683</p>
</blockquote>
<h3 id="使用批量梯度下降">使用批量梯度下降</h3>
<p>$8*500 = 4000 = 40 * 100$。 同样的时间内，批量梯度下降第50轮的损失远高于小批量梯度下降。可见下批量梯度下降优。</p>
<blockquote>
<p>Turn 400, loss is 0.00047387450467795134<br>
Turn 450, loss is 0.00041323609184473753<br>
Turn 500, loss is 0.0003657971683423966</p>
</blockquote>
<p>在拟合二次函数时，batch过小将导致欠拟合</p>
<h3 id="隐层宽度由3变4">隐层宽度由3变4</h3>
<p>一开始快一点</p>
<blockquote>
<p>Turn 400, loss is 0.0004357126308605075<br>
Turn 450, loss is 0.0003805999003816396<br>
Turn 500, loss is 0.000337266712449491</p>
</blockquote>
<h3 id="两隐层设置学习率">两隐层&amp;设置学习率</h3>
<p>第一隐层宽3，第二隐层宽2，效果显著</p>
<blockquote>
<p>Turn 400, loss is 9.72636480582878e-05<br>
Turn 450, loss is 8.47108312882483e-05<br>
Turn 500, loss is 7.491506403312087e-05</p>
</blockquote>
<p>第一隐层宽2，第二隐层宽2，500达到上方350轮效果</p>
<blockquote>
<p>Turn 400, loss is 0.00016509427223354578<br>
Turn 450, loss is 0.00014309250400401652<br>
Turn 500, loss is 0.00012597450404427946</p>
</blockquote>
<p>alpha = 1</p>
<blockquote>
<p>Turn 400, loss is 4.294736936572008e-05<br>
Turn 450, loss is 3.740956890396774e-05<br>
Turn 500, loss is 3.316783477202989e-05</p>
</blockquote>
<p>alpha = 10/100
直接到0</p>
<p>alpha = 1000
无法收敛，8.289306640625</p>
<h3 id="adam优化">Adam优化</h3>
<p>同样适用alpha = 0.5,其它参数默认值情况下轮前就达到了0.0</p>
<blockquote>
<p>Turn 0, loss is 0.5095147490501404<br>
Turn 50, loss is -0.0</p>
</blockquote>
<h1 id="实验15优化方法实验">实验1.5：优化方法实验</h1>
<h2 id="实验目标-1">实验目标</h2>
<p>在单隐层（宽3）、异或数据集上理解、实践以下优化算法：</p>
<ul>
<li>Gradient Descent</li>
<li>AdamDelta</li>
<li>AdaGrad</li>
<li>Momentum</li>
<li>Adam</li>
<li>RMSprop</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="实验结果-1">实验结果</h2>
<h3 id="深度学习83基本算法">《深度学习》8.3基本算法</h3>
<h4 id="sgd">SGD</h4>
<p>$g = gradient$<br>
$theta = theta - alpha * g$</p>
<h4 id="momentum">Momentum</h4>
<p>$g = gradient$<br>
$v = rou * v - alpha * g$<br>
$theta = theta + v$<br>
最大速度倍数为$\frac{1-rou}{1}$<br>
常用取值0.5,0.9,0.99</p>
<p>2倍时</p>
<blockquote>
<p>Turn 400, loss is 0.00015466260083485395
Turn 450, loss is 0.00013605377171188593
Turn 500, loss is 0.00012112771946704015</p>
</blockquote>
<p>10倍时，50轮之内达到0</p>
<h4 id="nesterov-动量">Nesterov 动量</h4>
<p>v = rou * v - alpha * g(theta+rou*v)
theta = theta + v</p>
<h3 id="参数自适应学习率算法">参数自适应学习率算法</h3>
<h4 id="adagrad">AdaGrad</h4>
<p>g = gradient<br>
r = $r + g^2$<br>
theta = theta - $\frac{alpha}{10^{-7}+\sqrt{r}}*g$<br>
缩放每个参数反比与其所有梯度历史平方总和的平方根，净效果是在参数空间中更为平缓的倾斜方向会取得更大的进步</p>
<blockquote>
<p>Turn 400, loss is 0.00014595562242902815<br>
Turn 450, loss is 0.00012745874118991196<br>
Turn 500, loss is 0.00011293470743112266</p>
</blockquote>
<h4 id="rmsprop">RMSProp</h4>
<p>加入了指数衰减<br>
g = gradient<br>
r = rou*r + (1-rou)* g^2<br>
theta = theta - $\frac{alpha}{10^{-7}+\sqrt{r}}$*g<br>
rou = 0.5 100轮0</p>
<h4 id="adadelta">AdaDelta</h4>
<p>g = gradient<br>
r = rho*r + (1-rho)*g^2<br>
RMS = $\sqrt{r}+alpha$<br>
theta = theta - $\frac{RMS_{t-1}}{RMS_{t}}$*g</p>
<h4 id="adam">Adam</h4>
<p>建议默认值： 0.001,0.9,0.999<br>
g = gradient<br>
s = $\frac{rou1<em>s+(1-rou1)<em>g}{1-rou1^2}$<br>
r = $\frac{rou2</em>r+(1-rou2)<em>g</em>g}{1-rou2^2}$<br>
theta = theta - alpha</em>$\frac{s}{\sqrt{r}+10^{-8}}$</p>
<h1 id="实验二mnist数据集实战">实验二：MNIST数据集实战</h1>
<h2 id="实验目标-2">实验目标</h2>
<h3 id="easy-1">EASY</h3>
<ul>
<li>完成带滑动平均、指数衰减、正则化的单隐层神经网络</li>
<li>完成tensorflow代码规范化</li>
<li>设置学习率相关参数</li>
<li>设置正则化相关参数</li>
<li>设置滑动平均参数</li>
<li>比较以上优化性能</li>
</ul>
<h3 id="hard-1">HARD</h3>
<ul>
<li>探索网络宽度和深度的影响</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="实验结果-2">实验结果</h2>
<h3 id="第一项性能">第一项性能</h3>
<p>batch = 100</p>
<blockquote>
<p>After 30000 training step(s), test accuracy using average model is 0.9837999939918518</p>
</blockquote>
<h3 id="规范化后">规范化后</h3>
<p>规范化后，train和eval两个过程分开进行。网络与上一个相同。在30000轮后：</p>
<blockquote>
<p>After 30001 training step(s), loss on training batch is 0.03692609444260597.<br>
After 30001 training step(s), validation accuracy = 0.9846000075340271<br>
After 30001 training step(s), test accuracy = 0.9843000173568726</p>
</blockquote>
<h3 id="正则化讨论">正则化讨论</h3>
<p>我们发现，如果不使用任何正则化，cross entropy下降的飞快：</p>
<blockquote>
<p>After 5001 training step(s), loss on training batch is 0.0019425891805440187.<br>
After 30001 training step(s), loss on training batch is 0.0003114454448223114.</p>
</blockquote>
<p>但是正确率缺不及0.0001L2正则化，并且很快就收敛了</p>
<blockquote>
<p>After 4001 training step(s), validation accuracy = 0.9832000136375427<br>
After 30001 training step(s), validation accuracy = 0.9837999939918518<br>
After 30001 training step(s), test accuracy = 0.9830999970436096</p>
</blockquote>
<p>我们尝试调节正则化参数。默认的参数是1e-4。我们首先估计一下数量级：</p>
<ul>
<li>不使用正则化时：MSE &ndash; 1e-3 to 1e-4</li>
<li>使用正则化系数1e-4时：Loss &ndash; 1e-2</li>
<li>参数 &ndash; 1e-1 to 1e-2</li>
<li>参数个数 &ndash; 1e5</li>
<li>L2范数 &ndash; 1e3 to 1e4</li>
</ul>
<p>我们估计正则化参数应在1e-3至1e-4间。若太大，其主导太强；若太小，影响太弱。</p>
<p>当lamda = 0.001时，模型前期下降速度明显减缓，2000轮后就出现损失率振荡现象</p>
<blockquote>
<p>After 2001 training step(s), loss on training batch is 0.21251729130744934.<br>
After 30001 training step(s), loss on training batch is 0.17756156623363495.</p>
</blockquote>
<p>8000轮后正确率不再提升</p>
<blockquote>
<p>After 8001 training step(s), validation accuracy = 0.9811999797821045<br>
After 30001 training step(s), validation accuracy = 0.9807999730110168<br>
After 30001 training step(s), test accuracy = 0.98089998960495
这时，性能反而下降。种种现象说明1e-3太大，使得模型欠拟合了</p>
</blockquote>
<p>当lamda = 0.00005时，<br>
First Test</p>
<blockquote>
<p>After 30001 training step(s), loss on training batch is 0.03040509857237339.<br>
After 30001 training step(s), validation accuracy = 0.9850000143051147<br>
After 30001 training step(s), test accuracy = 0.9829999804496765</p>
</blockquote>
<p>Second Test</p>
<blockquote>
<p>After 30001 training step(s), loss on training batch is 0.029661739245057106.<br>
After 30001 training step(s), validation accuracy = 0.984000027179718<br>
After 30001 training step(s), test accuracy = 0.9832000136375427
这个参数气起到了防止过拟合的效果，但不够彻底，不比0.0001优。</p>
</blockquote>
<p>当lamda = 0.0005时，也出现了振荡。这个数字可能还是稍大。</p>
<blockquote>
<p>After 30001 training step(s), loss on training batch is 0.11202621459960938.<br>
After 30001 training step(s), validation accuracy = 0.9847999811172485<br>
After 30001 training step(s), test accuracy = 0.9830999970436096</p>
</blockquote>
<p>如此lambda的大致范围已经确定了。我们尝试微调lambda，看看有什么好的效果。<br>
lambda = 0.0003，第9000轮Validation上的性能达到了0.985。</p>
<blockquote>
<p>After 30001 training step(s), loss on training batch is 0.06975933909416199.<br>
After 30001 training step(s), validation accuracy = 0.9861999750137329<br>
After 30001 training step(s), test accuracy = 0.9848999977111816
我们发现这次性能有小幅提升。为了确定不是欧气影响，我再试了一次，发现脱欧入非了。</p>
</blockquote>
<p>我们发现，在有效防止过拟合和避免欠拟合之间参数还有一定的选择范围。我们首先要确定这个可行域，再在可行域内微调到一个最好的值。
可行域内loss从1e-4退到1e-2，提高1e-3的精度，参数更改可以影响1e-4的精度。</p>
<p>我们尝试使用L1范数，果然出现了很多0值，并且参数范围约1e-1 to 1e-6。
我们先使用lambda = 0.0001，发现了振荡现象。效果并不理想</p>
<blockquote>
<p>After 30001 training step(s), loss on training batch is 0.14731723070144653.<br>
After 30001 training step(s), validation accuracy = 0.9807999730110168<br>
After 30001 training step(s), test accuracy = 0.9811000227928162</p>
</blockquote>
<p>我们再用lambda = 0.00001, 验证集上拟合不错，但是泛化较差。</p>
<blockquote>
<p>After 30001 training step(s), loss on training batch is 0.03477011248469353.<br>
After 30001 training step(s), validation accuracy = 0.9846000075340271<br>
After 30001 training step(s), test accuracy = 0.9819999933242798</p>
</blockquote>
<p>我们再用lambda = 0.000001,依然不够理想</p>
<blockquote>
<p>After 30001 training step(s), loss on training batch is 0.025545725598931313<br>
After 30001 training step(s), validation accuracy = 0.9847999811172485<br>
After 30001 training step(s), test accuracy = 0.9817000031471252</p>
</blockquote>
<p>当使用1e-7时，开始有效果</p>
<blockquote>
<p>After 30001 training step(s), loss on training batch is 0.0033931396901607513.<br>
After 30001 training step(s), validation accuracy = 0.9829999804496765<br>
After 30001 training step(s), test accuracy = 0.9833999872207642</p>
</blockquote>
<p>1e-8时几乎和没有使用正则化相同。1e-9完全没有效果了。故1e-7可能是一个号选择。</p>
<p>最后，我们尝试一下Elastic net regularization。我们分别用之前获得1e-4和1e-7作为参数：
我们开心的发现，效果很棒！</p>
<blockquote>
<p>After 30001 training step(s), loss on training batch is 0.03605485334992409.<br>
After 30001 training step(s), validation accuracy = 0.9846000075340271<br>
After 30001 training step(s), test accuracy = 0.9850999712944031</p>
</blockquote>
<p>为了确认是不是欧气作祟，我再一次启动。发现跟之前的又变的差不多了。那么姑且认为选好的参数的Elastic net 不差于L2 吧。</p>
<p>我们再尝试把两个参数减半，发现效果并不好。后面的测试都在L2正则化下进行。</p>
<h3 id="学习率">学习率</h3>
<p>接下来我们探讨学习率。目前的学习率为：base=0.8,decay=0.99,1000轮约更新两次。到30000轮时，learning rate 约为0.4
若decay = 0.9, 最后只有0.0008。<br>
decay = 0.96, 约为0.086。经测试，此时下降过慢<br>
我们尝试不适用指数衰减学习：0.9838</p>
<p>我们尝试使用Adam学习：</p>
<blockquote>
<p>After 30001 training step(s), loss on training batch is 0.04590621590614319<br>
After 30001 training step(s), validation accuracy = 0.98580002784729<br>
After 30001 training step(s), test accuracy = 0.9854999780654907</p>
</blockquote>
<p>这个结果出乎我的意料。我一开始以为Adam只会是搜索更快进行。但其保留的历史记录似乎使其loss下降不如Gradient Descent快，但是泛化效果却更好。我再尝试一次，发现虽然没有这么高的数值，但是仍然比原先好。
<strong>Why?</strong></p>
<p>当alpha从0.001变至0.01时发生振荡，学习效果不好。
当alpha为0.0001时，下降太慢，loss还处在0.05左右</p>
<p>我们再尝试使用带指数衰减的Adam学习,发现梯度下降到和之前一样的水平。由此之前Adam的步幅太大了。</p>
<blockquote>
<p>After 30001 training step(s), loss on training batch is 0.030896920710802078.<br>
After 30001 training step(s), validation accuracy = 0.9872000217437744<br>
After 30001 training step(s), test accuracy = 0.9850000143051147</p>
</blockquote>
<p>看了Adam的效果很好，但是还有两个参数不熟悉。我们暂时只能其推荐值0.9和0.999
注意：Adam和L1,L2一起的组合拳效果并不理想，有待探究</p>
<h3 id="滑动平均">滑动平均</h3>
<p>我们去除滑动平均，0.9839
使用后有小幅提升</p>
<h3 id="宽度">宽度</h3>
<p>我们加宽一倍隐层（500-&gt;1000）, 性能显然增加了。</p>
<blockquote>
<p>After 30001 training step(s), loss on training batch is 0.03701362386345863.<br>
After 30001 training step(s), validation accuracy = 0.9851999878883362<br>
After 30001 training step(s), test accuracy = 0.9848999977111816</p>
</blockquote>
<h3 id="深度">深度</h3>
<p>2隐层整流线性，训练速度下降<br>
<strong>Why?</strong> 当LEARNING_BASE = 0.5并使用指数衰减时，loss变为inf。但0.5可以收敛<br>
利用Adam,alpha = 0.01，收敛速度太慢<br>
alpha = 0.1, 指数衰减，Elastic Net 正则化， 无法收敛<br>
多层难以训练，效果难以体现。暂时无法很好的使用。</p>
<h1 id="实验25">实验2.5</h1>
<h2 id="实验目标-3">实验目标</h2>
<ul>
<li>尝试各类激活函数</li>
<li>实现dropout</li>
<li>了解径向基网络结构</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="实验结果-3">实验结果</h2>
<h3 id="激活函数">激活函数</h3>
<h4 id="线性整流-relu">线性整流 ReLU</h4>
<p>$g(z) = max(0,z)$<br>
Flaw: $x&lt;0$ 硬饱和</p>
<blockquote>
<p>After 30001 training step(s), loss on training batch is 0.03692609444260597.<br>
After 30001 training step(s), validation accuracy = 0.9846000075340271<br>
After 30001 training step(s), test accuracy = 0.9843000173568726</p>
</blockquote>
<h4 id="线性整流变种">线性整流变种</h4>
<p>$g(z,a) = max(0,z)+alpha*min(0,z)$</p>
<p>$leaky_relu(x) = \max(z,z*alpha)$ 即 alpha为小值<br>
Flaw: 均值没有保证 <em>Why?</em><br>
一样的学习率下loss 0.04，accuarcy = 0.982</p>
<p>relu6(x) = min(max(z,0),6) one kind of hard tanh<br>
crelu(x) = [ReLU(x),ReLU(-x)]<br>
ELU(x) = x : x&gt;0 = alpha*(e^x-1) : x&lt;0<br>
SELU(x)= lambda * ELU(x), lambda&gt;1</p>
<p>以上loss下降普遍偏慢</p>
<h4 id="softplus">softplus</h4>
<p>Relu 的平滑版本，下降更慢<br>
$g(z) = log(1+z^x)$</p>
<h4 id="sigmoid-and-tanh">sigmoid and tanh</h4>
<p>sigmoid(x) = $\frac{1}{1+e^{-1}}$<br>
tanh(x) = $\frac{1-e^{-2x}}{1+e^{-2x}}$</p>
<h3 id="dropout">dropout</h3>
<p>隐层抽样概率为0.5，输入层为0.8<br>
tensorflow 中: With probability keep_prob, outputs the input element scaled up by 1 / keep_prob, otherwise outputs 0. The scaling is so that the expected sum is unchanged.<br>
我们发现，dropout有效防止了过拟合，和L1,L2正则化一起，最后的loss很高。</p>
<blockquote>
<p>After 30001 training step(s), loss on training batch is 0.11102759093046188.<br>
After 30001 training step(s), validation accuracy = 0.9854000210762024<br>
After 30001 training step(s), test accuracy = 0.9839000105857849</p>
</blockquote>
<p>我们去掉所有正则化后,loss呈现震荡，但accuracy增加，最后效果很好！</p>
<blockquote>
<p>After 30001 training step(s), loss on training batch is 0.0919218510389328.<br>
After 30001 training step(s), validation accuracy = 0.9837999939918518<br>
After 30001 training step(s), test accuracy = 0.9851999878883362</p>
</blockquote>
<p>加上L2正则化后，性能与单有L2正则化相近。</p>
<h3 id="径向基网络">径向基网络</h3>
<p>径向基函数神经网络的优点：逼近能力，分类能力和学习速度等方面都优于BP神经网络，结构简单、训练简洁、学习收敛速度快、能够逼近任意非线性函数，克服局部极小值问题。？原因在于其参数初始化具有一定的方法，并非随机初始化。</p>
<h4 id="径向基函数">径向基函数</h4>
<ol>
<li>高斯函数 $\phi(r) = e^{-\frac{r^2}{2\sigma^2}}$</li>
<li>反演S型函数 $\phi(r) = \frac{1}{1+e^{\frac{r^2}{\sigma^2}}}$</li>
<li>拟多二次函数 $\phi(r) = \frac{1}{(r^2+****\sigma^2)^{1/2}}$</li>
</ol>
<p>其中，$\sigma$为基函数的扩展常数（宽度），越小则基函数的宽度越小，基函数就越具有选择性</p>
<ul>
<li>全局逼近网络： 可调参数对任何一个输出都有影响</li>
<li>局部逼近网络： 某个局部区域只有少数几个连接权重影响网络输出</li>
</ul>
<p>数学原理： 一般函数都可以表示为一组基函数的线性组合</p>
<h4 id="学习基函数中心">学习基函数中心</h4>
<ul>
<li>随机选取</li>
<li>自组织选取</li>
<li>有监督选取</li>
<li>正交最小二乘</li>
</ul>
<h4 id="学习">学习</h4>
<p>学习 中心、宽度和线性权值。将输入通过基函数映射到高维空间后用线性层划分</p>
<h1 id="实验三">实验三</h1>
<h2 id="实验目标-4">实验目标</h2>
<h3 id="easy-2">Easy</h3>
<ul>
<li>实现基本的卷积神经网络&ndash;LeNet-5模型</li>
<li>实现LSTM的简单结构</li>
<li>实现LSTM的语言预测，在数据集PTB上</li>
</ul>
<h3 id="hard-2">Hard</h3>
<ul>
<li>实现Inception-v3框架的迁移学习 （计算资源不足，暂时不做）</li>
<li>实现机器翻译模型</li>
</ul>
<!-- raw HTML omitted -->
<h3 id="实验结果-4">实验结果</h3>
<h4 id="lenet-5模型">LeNet-5模型</h4>
<p>训练又慢又耗电。1000轮要耗掉10%的点，半小时才不到100000轮
学习率设置过大很容易无法收敛，0.01都过大。0.001下降速度不够快，而且可能也是大。</p>
<p>改用Adam,用0.001的学习率，发现效果非常好。这说明Adam的鲁棒性不是盖的：</p>
<blockquote>
<p>After 501 training step(s), loss on training batch is 0.46121683716773987.<br>
After 501 training step(s), validation accuracy = 0.9872000217437744<br>
After 1001 training step(s), loss on training batch is 0.2894487977027893.</p>
</blockquote>
<p>2000轮后，就可以达到：</p>
<blockquote>
<p>After 2001 training step(s), loss on training batch is 0.20919263362884521.
After 2001 training step(s), validation accuracy = 0.9927999973297119</p>
</blockquote>
<h4 id="lstm">LSTM</h4>
<p>tensorflow 对LSTN的封装很好，代码简单</p>
<blockquote>
<p>train step: 9000, loss: 4.668378e-06<br>
Mean Square Error is: 0.002043843502178788</p>
</blockquote>
<h4 id="ptb">PTB</h4>
<blockquote>
<p>After 6500 steps, perplexity is 73.22564944927569<br>
After 6600 steps, perplexity is 72.36911348195412<br>
Epoch: 5 Train Perplexity: 72.57878706561621<br>
Epoch: 5 Eval Perplexity: 107.28163012316921</p>
</blockquote></main>
        </div>

    </div>
</body></html>