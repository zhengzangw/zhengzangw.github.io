<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CS 5242: Neural Networks and Deep Learning (NUS, Fall 2022) on Zangwei</title>
    <link>https://zhengzangw.com/teach/cs5242-2022-fall/</link>
    <description>Recent content in CS 5242: Neural Networks and Deep Learning (NUS, Fall 2022) on Zangwei</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>zhengzangw at gmail.com (Zangwei Zheng)</managingEditor>
    <webMaster>zhengzangw at gmail.com (Zangwei Zheng)</webMaster>
    <lastBuildDate>Wed, 07 Sep 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://zhengzangw.com/teach/cs5242-2022-fall/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A Detailed Back Propagation Deduction</title>
      <link>https://zhengzangw.com/teach/cs5242-2022-fall/bp/</link>
      <pubDate>Wed, 07 Sep 2022 00:00:00 +0000</pubDate>
      <author>zhengzangw at gmail.com (Zangwei Zheng)</author>
      <guid>https://zhengzangw.com/teach/cs5242-2022-fall/bp/</guid>
      <description>Outline There are several ways for calculation of BP. First, notice that we want to calculate $\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}}$ for each layer, which is a scalar-to-matrix derivative. We can use the following methods:
 Calculate $\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}}$ directly with matrix-to-matrix gradients. (not adopted here because matrix-to-matrix gradients are not easy to calculate) Calculate $\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}}$ directly and avoid vector-to-matrix gradients. (not adopted here, because it is still hard) Calculate $\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}_{i,j}}$ and then assemble them into a matrix.</description>
    </item>
    
  </channel>
</rss>
