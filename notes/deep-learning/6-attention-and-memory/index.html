<!DOCTYPE html>
<html lang="en-us">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>注意力机制与外部记忆 | Zangwei</title>

    
    <link rel="stylesheet" href="/scss/main.min.db87d7b55e29b75699acf73451f98e37f7029321133dd8ae45930563c6c76609.css" integity="sha256-24fXtV4pt1aZrPc0UfmON/cCkyETPdiuRZMFY8bHZgk=">

    <script type="text/javascript" src="/js/dark.js"></script>
<script>
  MathJax = {
    tex: {
      inlineMath: [["$", "$"]],
    },
    displayMath: [
      ["$$", "$$"],
      ["\[\[", "\]\]"],
    ],
    svg: {
      fontCache: "global",
    },
  };

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
></script>
</head><body class="app auto flex-container">
    <div class="flex-container flex-column"><nav>
    <hr>
    <div class="flex-container flex-row flex-row-full">
        
        <div class="nav-item">
            <a href="/about">[ About ]</a>
        </div>
        
        <div class="nav-item">
            <a href="/pdfs/resume.pdf">[ CV ]</a>
        </div>
        
        <div class="nav-item">
            <a href="/teach">[ Teaching ]</a>
        </div>
        
        <div class="nav-item">
            <a href="/posts">[ Posts ]</a>
        </div>
        
        <div class="nav-item">
            <a href="/notes">[ Notes ]</a>
        </div>
        
        <div class="nav-item">
            <a href="/friends">[ Friends ]</a>
        </div>
        
        <div class="nav-item btn btn-switch">
            <a>[ <span class="theme-name">Auto</span> ]</a>
        </div>
    </div>
    <hr>
</nav>
<div class="flex-passage flex-row flex-row-full">
            <div class="flex-column-20">
                <div class="return">
                    <a href=".."> RETURN </a>
                </div>
                <nav id="TableOfContents">
  <ul>
    <li><a href="#注意力机制">注意力机制</a>
      <ul>
        <li><a href="#自注意力模型">自注意力模型</a></li>
      </ul>
    </li>
    <li><a href="#记忆">记忆</a>
      <ul>
        <li><a href="#记忆增强神经网络mannmn">记忆增强神经网络（MANN/MN）</a></li>
        <li><a href="#基于神经动力学的联想记忆">基于神经动力学的联想记忆</a></li>
      </ul>
    </li>
  </ul>
</nav>
            </div>
            <main class="flex-column-80"><h2 id="注意力机制">注意力机制</h2>
<ul>
<li>认知神经科学中的注意力
<ul>
<li>聚焦式注意力</li>
<li>基于显著性的注意力</li>
<li>鸡尾酒会效应</li>
</ul>
</li>
<li>基于显著性的注意力机制：最大汇聚、门控</li>
<li>注意力分布：
<ul>
<li>输入变量：$[x_1,\cdots,x_N]$</li>
<li>查询变量：$q$</li>
<li>注意力变量：$z=n$ 选择第 $n$ 个输入变量</li>
<li>注意力打分函数：$s(x,q)$
<ul>
<li>加性模型：$s(x,q)=v^\top \tanh(Wx+Uq)$</li>
<li>点积模型：$s(x,q)=x^\top q$</li>
<li>缩放点积模型：$s(x,q)=\frac{x^\top q}{\sqrt{D}}$</li>
<li>双线性模型：$s(x,q)=x^\top Wq$</li>
</ul>
</li>
<li>注意力分布：$\alpha_n=p(z=n|X,q)=\text{softmax}(s(x_n,q))$</li>
</ul>
</li>
<li>软性注意力机制：$\text{att}(X,q)=\sum_{n=1}^N\alpha_nx_n$</li>
<li>硬性注意力机制：无法使用反向转播，通常用强化学习训练
<ul>
<li>$\text{att}=x_{\hat n},\hat n=\arg\max\alpha_n$</li>
<li>随机采样</li>
</ul>
</li>
<li>键值对注意力
<ul>
<li>输入信息：$[(k_1,v_1),\cdots,(k_N,v_N)]$</li>
<li>注意力函数：$\sum_{n=1}^N\frac{\exp(s(k_n,q))}{\sum_j\exp(s(k_j,q))}v_n$</li>
</ul>
</li>
<li>多头注意力：$\text{att}((K,V),Q)=\oplus\text{att}((K,V),q_i)$</li>
<li>结构化注意力</li>
<li>指针网络（2015）：序列到序列模型，输出下标
<ul>
<li>$p(c_{1:M}|x_{1:N})=\prod_{m=1}^M p(c_m|c_{1:(m-1)},x_{1:N})\approx\prod_{m=1}^Mp(c_m|x_{c1:c_{m-1}},x_{1:N})$</li>
<li>$p(c_m|c_{1:{(m-1)},x_{1:N}})=\text{softmax}(s_{m,n})$</li>
<li>$s_{m,n}=v^\top\tanh(Wx_n+Uh_m)$</li>
</ul>
</li>
</ul>
<h3 id="自注意力模型">自注意力模型</h3>
<ul>
<li>变长向量序列
<ul>
<li>卷积网络或循环网络编码</li>
<li>自注意力模型（内部注意力模型）</li>
</ul>
</li>
<li>如果要建立输入序列之间的长距离依赖关系，可以使用以下两种方法：一种方法是增加网络的层数，通过一个深层网络来获取远距离的信息交互，另一种方法是使用全连接网络</li>
<li>QKV 模式
<ul>
<li>输入序列：$X=[x_1,\cdots,x_N]\in\mathbb{R}^{D_x\times N}$</li>
<li>输出序列：$H=[h_1,\cdots,h_N]\in\mathbb{R}^{D_v\times N}$</li>
<li>查询向量：$q_i\in\mathbb{R}^{D_k},Q=W_qX$</li>
<li>键向量：$k_i,Q=W_qX\in\mathbb{R}^{D_k\times N}$</li>
<li>值向量：$v_i,V=W_vX\in\mathbb{R}^{D_v\times N}$</li>
<li>$h_n=\text{att}((K,V),q_n)$</li>
</ul>
</li>
</ul>
<h2 id="记忆">记忆</h2>
<ul>
<li>人脑记忆
<ul>
<li>整体效应储存（分布式）</li>
<li>周期性
<ul>
<li>长期记忆（结构记忆，知识）</li>
<li>短期记忆</li>
<li>工作记忆（约 4 组项目）</li>
</ul>
</li>
<li>联想记忆：基于联想检索</li>
</ul>
</li>
</ul>
<h3 id="记忆增强神经网络mannmn">记忆增强神经网络（MANN/MN）</h3>
<ul>
<li>基本模块
<ul>
<li>主网络（控制器）</li>
<li>外部记忆单元：分为多个记忆片段 $M=[m_1,\cdots,m_N]$</li>
<li>读取模块：根据主网络的查询向量 $q_r$，读取 $r=R(M,q_r)$</li>
<li>写入模块：根据主网络的查询向量 $q_\omega$ 和需写入信息 $a$ 更新 $M=W(M,q_\omega,a)$</li>
</ul>
</li>
<li>按内容寻址：注意力机制
<ul>
<li>$r=\sum_{n=1}^N\alpha_n m_n$</li>
<li>$\alpha=\text{softmax}(s(m_n,q_r))$</li>
</ul>
</li>
<li>端到端记忆网络（MemN2N,2015）：外部记忆只读
<ul>
<li>$m_{1:N}={m_1,\cdots,m_N}$</li>
<li>转换成两组记忆片段 $A,C$ 分别用来寻址和输出</li>
<li>$r=\sum_{n=1}^N\text{softmax}(a_n^\top q)c_n$</li>
<li>$y=f(q+r)$</li>
<li>多跳操作：$q^{(k)}=r^{(k-1)}+q^{(k-1)}$</li>
</ul>
</li>
<li>神经图灵机（2014）
<ul>
<li>外部记忆：$M\in\mathbb{R}^{D\times N}$</li>
<li>控制器：神经网络</li>
<li>每个时刻 $t$，接受 $x_t,h_{t-1},r_{t-1}$ 输出 $h_t$，生成查询向量 $q_t$，删除向量 $e_t$ 和增加向量 $a_t$</li>
<li>读操作：$\alpha_{t,n}=\text{softmax}(s(m_{t,n},q_t))$
<ul>
<li>读向量：$r_t=\sum_{n=1}^N\alpha_{t,n}m_{t,n}$</li>
</ul>
</li>
<li>写操作：$m_{t+1,n}=m_{t,n}(1-\alpha_{t,n}e_t)+\alpha_{t,n}\alpha_t$</li>
</ul>
</li>
</ul>
<h3 id="基于神经动力学的联想记忆">基于神经动力学的联想记忆</h3>
<ul>
<li>Hopfield 网络
<ul>
<li>状态：${+1,-1}$</li>
<li>更新规则：$s_i=s_i+\text{sgn}(\sum_{j=1}^M\omega_{ij}s_j+b_i)$</li>
<li>能量函数：$E=-\frac{1}{2}\sum_{i,j}\omega_{ij}s_is_j-\sum_ib_is_i$</li>
<li>权重对称：$\omega_{ii}=0,\omega_{ij}=\omega_{ji}$</li>
<li>稳定性：能量函数多次迭代后收敛</li>
<li>吸引点：稳定状态，局部最优点，有限，网络储存的模式</li>
<li>信息储存（学习规则）：赫布规则 $\omega_{ij}=\frac{1}{N}\sum_{n=1}^Nx_i^{(n)}x_j^{(n)}$</li>
<li>储存容量：数量为 $M$ 的二值神经元网络，总状态数为 $2^M$，有效稳定点状态数即储存容量
<ul>
<li>Hopfield: 0.14 $M$</li>
<li>玻尔兹曼机: 0.6 $M$，收敛较慢</li>
</ul>
</li>
</ul>
</li>
</ul>
</main>
        </div>

    </div>
</body></html>