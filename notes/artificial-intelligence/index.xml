<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>[A] 人工智能 Artificial Intelligence on Zangwei</title>
    <link>https://zhengzangw.com/notes/artificial-intelligence/</link>
    <description>Recent content in [A] 人工智能 Artificial Intelligence on Zangwei</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>zhengzangw at gmail.com (Zangwei (Alex) Zheng)</managingEditor>
    <webMaster>zhengzangw at gmail.com (Zangwei (Alex) Zheng)</webMaster>
    <lastBuildDate>Wed, 16 Jan 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://zhengzangw.com/notes/artificial-intelligence/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>1-概论</title>
      <link>https://zhengzangw.com/notes/artificial-intelligence/1-introduction/</link>
      <pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate>
      <author>zhengzangw at gmail.com (Zangwei (Alex) Zheng)</author>
      <guid>https://zhengzangw.com/notes/artificial-intelligence/1-introduction/</guid>
      <description>科学与技术关系的探讨 周光绍 科学：正确反映客观世界现象、物质内部结构和运动规律的系统理论知识 技术：在科学指导下，总结实践经验，各方面的系统知识 白春礼 科学：发现、探索研究事物运动的客观规律 技术：[做什么、怎么做 人工智能 强人工智能：人工智能使探索人脑与意识的科学 弱人工智能：模拟人脑相应功能且得到应用，偏向于技术 何为人工智能 像人一样行动 图灵测试 自然语言处理 知识表示 自动推理 机器学习 完全图灵测试 计算机视觉 机器人学 像人一样思考 领会人脑用途 内省：捕捉思维过程 心理实验 脑成像 认知科学 合理地思考 合理地行动 学科交叉 哲学 图灵测试(1950) 西尔勒中文屋子(1980)：能否从表现来评判智能 人为陷阱：能否真的了解人类的智能水平 缸中之脑(1981) 反向图灵测试(CAPTCHA,2002) 智能的哲学发展 笛卡尔(1596-1650)：可通过可编程机械实现智能 莱布尼茨(1646-1716)：不存在预先设置的机器智能 磨坊论证：知觉的存在是对智能的存在不可或缺的吗？ 霍布斯(1588-1679，符号流派哲学先驱)：机械化的心灵观，仅仅限制在理性（推理） 休谟(1711-1776，联结流派哲学先驱)：心智模型是“自底而上”构建的，最底层是感觉，其次是抽象和记忆，再上是想象 康德(1724-1804)：心智模型是知觉的“从上(知性)至下(高阶 知觉)” 和“自下(感性)而上(低阶知觉)”,两者不可或缺 霍布斯·德瑞福斯(1929-)：真实的思维是不可以被程序穷尽，依赖计算力的“智能”不是人类智能 罗德尼·布洛克斯（行为流派先驱）：世界是最大的认知模型，更重要的是如何由嵌入式的方式认知外部世界，而并不需要创造中间符号 乔姆斯基 大脑是“认知”的 大部分智力活动是“先天”的 模式是大脑认知的关键结构 数学 经济学 囚徒困境 投票悖论 冷扑大师：博弈均衡策略求解算法 神经科学 心理学 计算机工程 控制论 自动化 语言学 Spaif-Whorf 假说：语言决定人们对世界的理解 大数据 4V:Volume,Velocity,Variety,Veracity 多源异构数据 海量分布数据 流数据 交互型数据 图数据 4I:Inexact,Incremental,Integrated,Inductive 大数据的表示模型：数据表示如何充分体现大数据的内在规律 大数据的计算模型：超越样本多项式时间复杂度的大数据计算模型 在线学习 近似学习 大数据的决策模型：面向多个行为实体的大数据复杂决策技术（数据分析到博弈推理） 历史发展 孕育期(1943-1955) 达特茅斯会议(1956) 克劳德·香农 约翰·麦卡锡 搜索与剪枝 马文·明斯基 知识表示和推理 奥利弗·赛弗里奇 赫伯特·西蒙 &amp;amp; 阿兰·纽维尔 推理期(1952-1969) 第一次低潮 早期程序对其主题一无所知 求解问题的难解性；组合爆炸 流派之争 Perceptrons 符号学派（逻辑） 连接学派（仿生） 行为学派（控制） 知识期(1970-1980 末) 第二次高潮（符号学派成功） 专家系统 第二次低潮（五代机计划失败） 学习期 第三次高潮（机器学习的成功） 深度期 新一代人工智能五个方向 大数据智能 群体智能 跨媒体智能 混合增强智能 自主无人系统 智能 Agent 理性 依赖于：性能度量，先验知识，可能的行动，感知序列 对环境的先验知识 可以完成的行动：执行器 截止此时的感知序列：信息收集，传感器 期望性能最大化 完美：实际性能最大化 任务环境 = (Performance Measurement,Environment,Actuators,Sensors) PEAS 任务环境的性质 Observable: 传感器是否能在每个时间点上获得完整信息 Multi Agents: 存在其它 Agents Deterministic: 下一状态是否完全决定于当前转态与行为 Episodic Static or Dynamic Dynamic: Agent 计算时环境会变化 Semi: 环境不变，但性能评价变化 离散/连续 环境已知/未知 Agent 函数：将感知信息映射到行动 Agent 程序：实现 Agent 函数 Agent = 体系结构（执行器，传感器，计算装置） + Agent 程序 Agent 程序 Table Driven Agent Simple Reflex Agent RULE-MATCH 仅使用当前状态 Model Based Reflex Agent 拥有内部状态（模型） 使用当前状态与内部状态 Agent with Goal 不使用 RULE-MATCH 确定当前状态与内部状态及动作后果，与目标比较后选择执行 Agent with Utility 学习 Agent 表达能力 原子表示：单点 要素化表示：集合 结构化表示：逻辑等 解决问题 CV 人脸识别 姿态识别 NLP 自然语言生成 语音识别 虚拟助理 IR 推荐系统 搜索引擎 交叉 机器人 决策管理 自动驾驶 生物：基因组 医疗：辅助决策、医学信息分析 金融：量化交易、安全、服务 网络安全 教育 </description>
    </item>
    
    <item>
      <title>2-搜索</title>
      <link>https://zhengzangw.com/notes/artificial-intelligence/2-search/</link>
      <pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate>
      <author>zhengzangw at gmail.com (Zangwei (Alex) Zheng)</author>
      <guid>https://zhengzangw.com/notes/artificial-intelligence/2-search/</guid>
      <description>无信息搜索 问题定义：初始状态，可能行动，转移模型，目标测试，路径耗散 参数 分支因子 $d$：每个状态有 $d$ 个后继 最优解代价 $C^*$ 每个行动代价至少为 $\epsilon$ 任一结点最大深度 性能度量 完备性：有解则一定能找到 最优性：能找到最优解 时间复杂度 空间复杂度 标准 宽度优先 一致代价 深度优先 深度受限 迭代加深 双向 完备性 $b&amp;lt;\infty$ $b&amp;lt;\infty$ No No $b&amp;lt;\infty$ $b&amp;lt;\infty$ 最优性 单步代价相同 Yes No No Yes 双向为宽度优先 时间复杂度 $O(b^d)$ $O(n^{1+\frac{C^*}{\epsilon}})$ $O(b^m)$ $O(b^l)$ $O(b^d)$ $O(b^{d/2})$ 空间复杂度 $O(b^d)$ $O(n^{1+\frac{C^*}{\epsilon}})$ $O(bm)$ $O(bl)$ $O(bd)$ $O(b^{d/2})$ 启发式搜索 a heuristic is a robust technique for the design of randomized algorithms for Optimziation Problems not able to guarantee the efficiency and the qulity of the computed feasible solution 评价函数: $f(n)$ 已花费的代价：$g(n)$ 启发函数：$h(n)=$结点$n$到目标结点最小代价的估计值 贪婪优先搜索 $f(n)=h(n)$</description>
    </item>
    
    <item>
      <title>3-推理（演算）</title>
      <link>https://zhengzangw.com/notes/artificial-intelligence/3-deduction/</link>
      <pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate>
      <author>zhengzangw at gmail.com (Zangwei (Alex) Zheng)</author>
      <guid>https://zhengzangw.com/notes/artificial-intelligence/3-deduction/</guid>
      <description>推理 推理规则 完备性 completeness 可靠性 soundness 推理算法 = 推理规则 + 搜索算法 完备的推理算法 = 完备推理规则 + 完备搜索算法 反证法：证明 $a\vDash b$，只需证 $a\wedge\neg b$ 不可满足 单元归结：$\frac{l_1\vee\cdots\vee l_k, m}{l_1\vee\cdots\vee l_{i-1}\vee l_{j+1}\vee\cdots\vee l_k}$ $l_k$ 与 $m$ 为互补文字 全归结：$\frac{l_1\vee\cdots\vee l_k, m_1\vee\cdots\vee m_k}{l_1\vee\cdots\vee l_{i-1}\vee l_{j+1}\vee\cdots\vee l_k\vee m_1\cdots}$ 归并：去除文字的多余副本 归结推理：$\frac{l_1\vee\cdots\vee l_k, m_1\vee\cdots\vee m_k}{l_1\vee\cdots\vee l_{i-1}\vee l_{j+1}\vee\cdots\vee l_k\vee m_1\cdots[x/\theta]},\text{UNIFY}(l_i,\neg m_j)=\theta$ 基本归结定理：如果子句集是不可满足的，那么这些子句的归纳闭包包含空子句 Horn 子句：至多只有一个正文字的子句 限定子句：恰有一个正文字的子句 每个限定子句可以写成蕴含式 目标子句：没有正文字的子句 Horn 子句在归结下封闭 转为和取反式(CNF) 命题逻辑处理 消去 $\Leftrightarrow,\Rightarrow$ 否定内移 变量标准化，量词左移 Skolem 化：消除存在量词 $(\exists x)(A(x))$ 置换为 $A(c)$ 删除全称量词 将$\wedge$分配到$\vee$中 一阶逻辑的命题化技术 (1960s) $\forall$ 消除 + Skolem 化（$\exists$ 消除） 有限：可满足时等价 无限：Jacques Herbrand 存在最大嵌套深度 一阶逻辑的蕴含问题是半可判定的 不存在算法否定蕴含不成立的语句 置换：$\theta={t_1/x_1,t_2/x_2,\cdots,t_n/x_n}$ 置换的复合：$\theta\circ\lambda$ 数据库语义：没提到的为假 画面问题：作为动作的结果，什么变什么不变 规约（消解，归结，resolution） 使用最少的合一次数在一个子句数据库中发现矛盾</description>
    </item>
    
    <item>
      <title>4-知识表示</title>
      <link>https://zhengzangw.com/notes/artificial-intelligence/4-knowledge/</link>
      <pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate>
      <author>zhengzangw at gmail.com (Zangwei (Alex) Zheng)</author>
      <guid>https://zhengzangw.com/notes/artificial-intelligence/4-knowledge/</guid>
      <description>知识表示 数据：信息的载体和表示 信息：数据的语义 知识：信息关联后形成的信息结构（事实和规则） 相对正确性 可表示/利用性 不确定性 一阶谓词表示(First Order Predicate) 一阶逻辑的知识工程 确定任务 搜集相关知识 确定词汇表，包括谓词，函词和常量 对邻域通用知识编码 对特定问题编码 提交查询给推理过程并获取答案 知识库调试 谓词公式表示知识的步骤 定义谓词及变元 变元赋值 连接词连接谓词，形成谓词公式 状态谓词 初始状态 终止状态 操作谓词 条件 状态改变 缺点：组合爆炸 产生式表示(Production) $P\rightarrow Q$ (IF P THEN Q) 产生式的知识有可信度 常用结构 原因$\rightarrow$结果 条件$\rightarrow$结论 前提$\rightarrow$操作 事实$\rightarrow$进展 情况$\rightarrow$行为 产生式系统：一组产生式互相配合协调 规则库 数据库 控制系统 推理 正向推理 反向推理 混合驱动和控制策略 语义网络表示(Semantic Network) 通过有向图，其顶点表示概念，边表示概念间的语义关系，来表达复杂的概念及其相互关系 语义网路的推理：带求解问题构造为网络判断，某些节点/边为空（询问点），进行匹配 框架表示(Framework) 与知识图谱(Knowledge Graph) 框架：描述对象属性的一种数据结构 框架的一般表示 框架名 槽名（某一方面属性） 侧面（属性的某一方面） 值 万维网(World Wide Web) Web1.0 URL: 统一资源定位符 HTML：超文本标记语言 HTTP：超文本传输协议 Web 搜索引擎 Web 2.</description>
    </item>
    
    <item>
      <title>5-强化学习</title>
      <link>https://zhengzangw.com/notes/artificial-intelligence/6-reinforce-learning/</link>
      <pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate>
      <author>zhengzangw at gmail.com (Zangwei (Alex) Zheng)</author>
      <guid>https://zhengzangw.com/notes/artificial-intelligence/6-reinforce-learning/</guid>
      <description>强化学习 交互学习：通过交互学习一个目标，Trial and Error 状态/奖励的分布式是策略依赖的 model-base: $V$ 根据已知数据计算，如动态规划 model-free: 取样试验得到 在线学习：如 Sarsa 离线学习：如 Q Learning Markov Decision Process $S$: 状态集合 $A$: 动作集合 $R:S\times A\rightarrow\mathbb{R}$: 即时奖励函数 $\delta:S\times A\times S\rightarrow \mathbb{R}$: 状态转移概率 轨迹：经验/情节(episode) 策略 $\pi:S\rightarrow A$ $\pi:S\times A\rightarrow\mathbb{R}$ 单状态学习，$\pi^*(s)=\argmin_\pi R(s,\pi(s))$ 多状态学习：$\pi^*(s)=\argmin_\pi V^\pi(s)$ 有限状态：$R_t=\sum_{1\leq i\leq N}R(s_i,a_i)$ 无限状态 有折扣：$\sum_{i=0}^\infty\gamma^iR(s_i,a_i)$ 无折扣：$\lim_{N\rightarrow\infty}\frac{1}{N}\sum_{i=0}^{N-1}R(s_i,a_i)$ 值函数 $V^\pi(s)=\mathbb{E}_\pi[R_t|s_t=s]$ $Q^\pi(s,a)=\mathbb{E}_\pi[R_t|s_t=s,a_t=a]$ $\pi$ 为最优策略 $\iff\forall s,V^(s)=V^{\pi^}(s)=\max_\pi V^{\pi}(s)=\max_a Q^{\pi}(s,a)$ Bellman 等式：$V^{\pi}(s)=\mathbb{E}\pi[R_t+\gamma V^\pi(s{t+1})|s_t=s]$ $V^\pi(s)=\sum_{a\in A}\pi(a|s)[R(s,a)+\gamma\sum_{s&amp;rsquo;}\delta(s,\pi(s),s&amp;rsquo;)V^{\pi}(s&amp;rsquo;)]$ $Q^\pi(s,a)=R(s,a)+\gamma\sum_{s&amp;rsquo;}\delta(s,a,s&amp;rsquo;)V^\pi(s&amp;rsquo;)$ 动态规划方法 策略评估（Policy Evaluation）：MDP 已知，给定一个策略 $\pi$，评估返回值 有限状态：求解方程组 策略迭代：初始 $V^\pi(s)$ 后根据 Bellman 等式更新迭代 最优控制（Optiomal Control）：MDP 已知，寻找一个最优策略 $\pi^*$ 策略迭代算法 值迭代：$V^{i+1}(s)\leftarrow\max_a Q^\pi(s,a)$ 策略迭代</description>
    </item>
    
    <item>
      <title>5-符号学习</title>
      <link>https://zhengzangw.com/notes/artificial-intelligence/5-learning/</link>
      <pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate>
      <author>zhengzangw at gmail.com (Zangwei (Alex) Zheng)</author>
      <guid>https://zhengzangw.com/notes/artificial-intelligence/5-learning/</guid>
      <description>符号学习 又称样例学习，概念学习，归纳推理
实例集合：$X$ 目标概念：$c:X\rightarrow{0,1}$ 假设空间 $H$: $h:X\rightarrow{0,1}$ 概念学习：寻找假设 $h$，使得 $\forall x\in X,h(x)=c(x)$ 归纳学习假设：任一假设如果在足够大的训练样例集合中能很好的逼近目标概念函数，它也能在未见实例中很好的逼近目标概念 $h_j\geq h_k\iff (\forall x\in X)(h_k(x)=1\rightarrow h_j(x)=1)$ 一致：$\text{consistent}(h,D)=(\forall\langle x,c(x)\rangle\in D)h(x)=c(x)$ 变型空间（版本空间 version space）：$\text{VS}_{H,D}={h\in H|\text{consistent}(h,D)}$ 极大泛化：$G={g\in H|\text{consistent}(g,D)\wedge(\neg\exists g&amp;rsquo;\in H)(g&amp;rsquo;&amp;gt;g\wedge \text{consistent}(g&amp;rsquo;,D))}$ 极大特化：$S={s\in H|\text{consistent}(s,D)\wedge(\neg\exists s&amp;rsquo;\in H)(s&amp;rsquo;&amp;gt;s\wedge \text{consistent}(s&amp;rsquo;,D))}$ 表示定理：$\text{VS}_{H,D}={h\in H|(\exists s\in S)(\exists g\in G)(g\geq h\geq s)}$ 归纳推理/机器学习的根本问题 目标概念假设不在假设空间怎么办? 能设计包含所有假设的空间吗? 假设空间大小对未见实例的泛化能力有什么影响? 假设空间大小对所需训练样例数量有什么影响? 有偏学习 作为搜索的概念学习，假设空间为合取式表示
Find-S 算法（寻找极大特殊假设） $h(a_1,\cdots,a_n)=$最特殊的假设 for $x\in X,c(x)=1$ for $a_i$ if $x$ 不满足 $a_i$ 则将 $h$ 中的 $a_i$ 替换为 $x$ 满足的另一个最一般的约束 列表消除算法 for $\langle x,c(x)\rangle\in D$ $H=H\backslash{h\in H|h(x)\not=c(x)}$ 候选消除算法：正例搜索 $S$，反例缩小 $G$ 如果 $d$ 是一正例 从 $G$ 中移去所有与 $d$ 不一致的假设 对 $S$ 中每个与 $d$ 不一致的假设 $s$ 从 $S$ 中移去 $s$ 把 $s$ 的所有的极小泛化式 $h$ 加入到 $S$ 中，其中 $h$ 满足 $h$ 与 $d$ 一致，而且 $G$ 的某个成员比 $h$ 更一般 从 $S$ 中移去所有这样的假设：它比 $S$ 中另一假设更一般 如果 $d$ 是一个反例 从 $S$ 中移去所有与 $d$ 不一致的假设 对 $G$ 中每个与 $d$ 不一致的假设 $g$ 从 $G$ 中移去 $g$ 把 $g$ 的所有的极小特化式 $h$ 加入到 $G$ 中，其中 $h$ 满足 $h$ 与 $d$ 一致，而且 $S$ 的某个成员比 $h$ 更特殊 从 $G$ 中移去所有这样的假设：它比 $G$ 中另一假设更特殊 无偏学习 析取表示，幂集</description>
    </item>
    
    <item>
      <title>7-概率图</title>
      <link>https://zhengzangw.com/notes/artificial-intelligence/7-pgm/</link>
      <pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate>
      <author>zhengzangw at gmail.com (Zangwei (Alex) Zheng)</author>
      <guid>https://zhengzangw.com/notes/artificial-intelligence/7-pgm/</guid>
      <description>图模型 $p(x)=\prod_{k=1}^{K}p(x_k|x_1,\cdots,x_{k-1})$ 联合概率表：需要 $2^K-1$ 个参数 图模型基本问题 表示问题：如何用图结构描述变量间的依赖关系 学习问题：结构学习，参数学习 推断问题：已知部分变量，求其它变量条件分布概率 有向图模型：贝叶斯网(信念网, Judea Pearl) 无向图模型：马尔可夫网 有向图模型 $B=\langle G,\Theta\rangle$ $G$: 有向无环图 (DAG) $\Theta$: 条件概率表 (CPT) $\theta_{x_i|\pi_i}=P_B(x_i|\pi_i)$ 联合分布：$P(x_1,\cdots,x_n)=\prod_{i=1}^nP(x_i|\text{Parent}(x_i))$ 独立性判断 三变量间的典型依赖关系 同父结构: $x_3\perp x_4|x_1$ 顺序结构：$y\perp z|x$ 冲撞结构(V-structure): $x$⫫$y|z$ marginal independece: $z$ 完全未知则独立，给定 $z$ 则不独立 有向分离 (D-seperation，或道德化) 转化为无向图 (moral graph) 有向边改无向边 V 型结构两个父节点之间加边 $x$ 与 $y$ 有向分离：$\exists Z$, $x,y$ 在 $G\backslash Z$ 中分属两个连通分支则 $x\perp y|Z$ 马尔科夫覆盖 常见有向图模型 Sigmoid 信念网络 变量取值为 ${0,1}$ $p(x_k=1|x_{\pi_k};\theta)=\sigma(\theta_0+\sum_{x_i\in x_{\pi_k}}\theta_ix_i)$ $M$ 个父节点 表格记录：$2^M$ 参数 参数化模型：$M+1$ 参数 朴素贝叶斯分类器: 条件独立性假设 隐马尔科夫网 结构学习 搜索最优贝叶斯网络结构: $\text{NP}$-hard 评分搜索：$G^*=\arg\max_G g(G:D)$ MDL(最小描述长度)：综合编码长度最短的网络 $s(B|D)=f(\theta)|B|-\text{LL}(B|D)$ $\text{LL}(B|D)=\sum_{i=1}^m\log P_B(x_i)$ AIC 准则: $f(\theta)=1$ BIC 准则: $f(\theta)=\frac{\log m}{2}$ 常用策略 贪心法 网络结构约束 无向图模型 马尔科夫网： $V$: 随机变量 $E$: 变量间的依赖关系 $x_{{k}}$: 第$k$个团随机变量 无向图为马尔科夫网络 $\iff$ 满足以下三条性质之一 Pairwise Markov Property: $X_u\perp X_v|X_{V\backslash{u,v}},{u,v}\not\in E$ Local Markov Property: $X_v\perp X_{V\backslash\text{ne}(v)}|X_{\text{ne}(v)}$ $\text{ne}(v)$: 邻居，马尔科夫毯 Global Markov Property: $X_A \perp X_B|X_S$，$A,B$ 两个子集间任何一条路径都经过子集$S$，则给定 $S$ 后，$A,B$ 两个子集相互条件独立 构造 如果满足 $P\not\vDash(X\perp Y|V-{X,Y})$，则 $X,Y$ 加边 给定 $X$ 的马尔可夫毯，$X$ 独立于余下的变量 Hammersley-Clifford 定理：$p(x)&amp;gt;0$ 满足局部马尔科夫性质当且仅当可表为马尔科夫网上的吉布斯分布 吉布斯分布：$p(x)=\frac{1}{Z}\prod_{c\in\mathcal{C}}\phi_c(x_c)$ 势函数：$\phi_k(x_{{k}})$ 配分函数：$Z=\sum_{x\in \mathcal{X}}\prod_{c\in\mathcal{C}}\phi_c(x_c)$ 势函数：$f_k(x_{{k}})=e^{\omega_k^T\phi_k(x_{{k}})}$ 一般定义为玻尔兹曼分布 $\phi_c(x_c)=\exp(-E_c(x_c))$ 常见无向图模型 对数线性模型（最大熵模型） $\phi_c(x_c|\phi_c)=\exp(\theta_c^Tf_c(x_c))$ $\log p(x;\theta)=\sum_{c\in\mathcal{C}}\theta_c^Tf_c(x_c)-\log Z(\theta)$ 条件最大熵模型（Softmax 回归模型）：$p(y|x;\theta)=\frac{\exp(\theta^Tf(x,y))}{\sum_{y}\exp(\theta^Tf_y(x,y))}$ 条件随机场（CRF）：$p(y|x;\theta)=\frac{1}{Z(x;\theta)}\exp(\sum_{c\in\mathcal{C}}\theta_c^Tf_c(x,y_c))$ 线性链条件随机场 马尔科夫逻辑网 (MLN) = Markov Net + 一阶逻辑 判断一个知识库中是否包含公式 $F$，$F$ 在所有满足知识库的世界中恒真 公式附加权值的一阶逻辑知识库 基本思想：将一阶逻辑的限制放松，即一个可能世界违反公式越多，其发生的概率越小 $L$：${(F_i,w_i)}$ $F_i$: 一阶逻辑闭规则（无变元） $w_i$: 实数表示权重 $C$: 有限常数集 马尔科夫逻辑网 $M_{L,C}$ $L$ 中的任意闭原子对应一个二值结点，真为 $1$，假为 $0$ $L$ 任意闭规则对应一个特征值，若规则为真，特征值为 $1$ 否则为 $0$。权重为 $\omega_i$ $P(X=x)=\frac{1}{Z}\text{exp}(\sum_{i=1}^F\omega_in_i(x))$ $n_i$: $F_i$ 在 $X$ 中所有取真值的公式的数量 参数学习 无隐变量 有向图：$L(D,\theta)=\frac{1}{N}\sum_{n=1}^N\sum_{k=1}^K\log p(x_k^{(n)}|x_{\pi_k}^{(n)};\theta_k)$ 无向图：$L(D,\theta)=\frac{1}{N}\sum_{n=1}^N\log p(\sum_{c\in\mathbb{C}}\theta_c^Tf_c(x_c^{(n)}))-\log Z(\theta)=E_{x\sim\tilde p(x)}f_c(x_c)-E_{x\sim p(x;\theta)}f_c(x_c)=\frac{1}{N}\sum_{n=1}^Nf_c(x_c^{(n)})-\sum_xp(x;\theta)f_c(x_c)$ 采样近似第二个期望 坐标上升法：固定其它参数，优化一个参数 有隐变量 EM 算法 推断 条件概率查询：$p(q|e)=\frac{\sum_z p(q,e,z)}{\sum_{q,z}p(q,e,z)}$ 因果推理 已知网络中的祖先节点而计算后代节点的条件概率 诊断推理 已知后代节点计算祖先节点，贝叶斯定理 支持推理 原因间的相互影响 精确推断 精确推断：NP-hard 变量消除法 信念传播算法（BP, 合积算法，消息传递算法） 链上的 BP 算法 $p(x_t)=\frac{1}{Z}\prod_{t=1}^{T-1}\phi(x_t,x_{t+1})$ $p(x_t)=\frac{1}{Z}\sum_{x_1}\cdots\sum_{x_{t-1}}\sum_{x_{t+1}}\cdots\sum_{x_T}\prod_{t=1}^{T-1}\phi(x_t,x_{t+1})$ $p(x_t)=\frac{1}{Z}\mu_{t-1,t}(x_t)\mu_{t+1,t}(x_t)$ $\mu_{t-1,t}(x_t)=\sum_{x_{t-1}}\phi(x_{t-1},x_t)\mu_{t-2,t-1}(x_{t-1})$ $\mu_{t+1,t}(x_t)=\sum_{x_{t+1}}\phi(x_t,x_{t+1})\mu_{t+2,t+1}(x_{t+1})$ 树结构上的 BP 算法 图结构上的 BP 算法：联合树算法 近似推断 环路信念传播：使用信念传播算法在含环图上获得近似解 变分推断 变分推断（变分贝叶斯）：寻找简单分布 $q^*(z)$ 近似条件概率密度 $p(z|x)$ 泛函优化问题：$q^*(z)=\argmin_{q(z)\in Q}\text{KL}(q(z)|p(z|x))=\arg\max_{q(z)\in Q}\text{ELBO}(q,x)$ 候选分布族 $Q$ 平均场分布族：$q(z)=\prod_{m=1}^Mq_m(z_m),z_m\subseteq Z$ 神经网络拟合 $p(z|x)$ $\text{ELBO}(q,x)=\int \prod_{m=1}^Mq_m(z_m)(\log p(x,z)-\sum_{m=1}^M\log q_m(z_m))dz=\int q_j(\int\prod_{m\neq j}q_m\log p(x,z)dz_m)dz_j-\int q_j\log q_j dz_j+C$ $\text{ELBO}(q,x)=\int q_j\log \tilde p(x,z_j)dz_j-\int q_j\log q_j dz_j+C$ $\log \tilde p(x,z_j)=E_{q(z_{\backslash j})}\log p(x,z) + C$ 关于 $z_j$ 未归一化分布 最优化 $q_j^*(z_j)=\tilde p(x,z_j)$ 坐标上升法：迭代优化 $q_j^*(z_j),j=1,\cdots,M$，使其收敛到局部最优解 采样法 随机采样：cdf 递增，则在 cdf 逆函数上进行均匀采样 拒绝采样 未归一化分布 $\hat p(x)$ 采样样本 $\hat x$ 提议分布（参考分布） $q(x),\exists k,\forall x,kq(x)\geq \hat p(x)$ 接受概率：$\alpha(\hat x)=\frac{\hat p(\hat x)}{kq(\hat x)}$ 重要性采样：从参考分布中采样 $E_p[f(x)]=\int_x f(x)p(x)dx=E_q(f(x)\omega(x)]$ 重要性权重：$\omega(x)=\frac{p(x)}{q(x)}$ 未归一：$E_p(f(x))=\frac{\int_x\hat p(x)f(x)dx}{\int_x\hat p(x)dx}\approx\frac{\sum_{n=1}^Nf(x^{(n)}\hat\omega(x^{(n)}))}{\sum_{n=1}^N\hat\omega(x^{(n)})}$ 拒绝采样和重要性采样的效率随空间维数的增加而指数降低 MCMC 方法采样 预烧期（Burn-in Period） 相邻样本高度相关：每间隔 $M$ 次随机游走取一个样本 Metropolis-Hastings 算法 根据 $q(x|x_t)$ 采样 $\hat x$ 以如下概率接受：$A(\hat x,x_t)=\min(1,\frac{p(\hat x)q(x_t|\hat x)}{p(x_t)q(\hat x|x_t)})$ Metropolis 算法：MH 算法中提议分布对称 $A(\hat x,x_t)=\min(1,\frac{p(\hat x)}{p(x_t)})$ Gibbs Sampling：使用全条件概率作为提议分布，$A=1$ $p(x_m|x_{\backslash m})=p(x_n|x_1,\cdots,x_{m-1},x_{m+1},\cdots,x_M)$ 按下标顺次采样 </description>
    </item>
    
    <item>
      <title>8-博弈论</title>
      <link>https://zhengzangw.com/notes/artificial-intelligence/8-game-theory/</link>
      <pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate>
      <author>zhengzangw at gmail.com (Zangwei (Alex) Zheng)</author>
      <guid>https://zhengzangw.com/notes/artificial-intelligence/8-game-theory/</guid>
      <description>博弈论 策略 囚徒困境 布雷斯悖论（Braess&amp;rsquo;s paradox） 最优策略 帕里托优(Pareto Efficiency) 不存在另一个方案 $x&amp;rsquo;$, $\exists t,\text{UTILITY}{x&amp;rsquo;}(t)&amp;gt;\text{UTILITY}{x}(t),\forall t,\text{UTILITY}{x&amp;rsquo;}(t)\geq\text{UTILITY}{x}(t)$ 社会福利：和最优 Nash 均衡：没有参与者可以独自行动而增加收益 无纯策略纳什均衡解 多纳什均衡解 优超(Dominant)：不依赖其它参与者 投票 投票 $A$: Agent 集合 $O$: 投票结果集合 投票结果：$\succ_i\subseteq O\times O$ 社会偏序：$\succ^*$ 对任意 $o,o&amp;rsquo;\in O$ 有定义 非对称且传递 帕里托优：$\forall i,o\succ_io&amp;rsquo;\Rightarrow o\succ^*o&#39;$ 投票方案对不相关的候选人是独立的 没有 Agent 可以独裁 阿罗不可能定理 投票方案：使结果帕里托优 多数投票：所有候选人同时进行比较，得票最高者获胜 不满足无关方案独立原则 二叉投票 不满足无关方案独立原则 与序相关 计分投票：排名第一的得 $|O|$ 分，排名第二的得 $|O|-1$ 分，以此类推 不满足无关方案独立原则 拍卖 拍卖方案：使拍卖者增加自己的利益 英格兰拍卖：first-price open-cry 密封拍卖：first-price sealed-bid Vickery 拍卖：second-price sealed-bid 荷兰式拍卖：减价式拍卖 谈判 谈判方案 公理谈判机制：不变性、对称性、无关性、帕里托优 策略谈判机制 物品的折扣因素: $\pi_1=\frac{1-\delta_2}{1-\delta_1\delta_2}$ 谈判的代价因素 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://zhengzangw.com/notes/artificial-intelligence/9-information-theory/</link>
      <pubDate>Tue, 09 Jun 2020 00:00:00 +0000</pubDate>
      <author>zhengzangw at gmail.com (Zangwei (Alex) Zheng)</author>
      <guid>https://zhengzangw.com/notes/artificial-intelligence/9-information-theory/</guid>
      <description>基本概念 随机变量：$S$ 信息：消除随机变量不确定性的事物 信息量与传播媒介无关 信息是相对的 信息是客观物理量 噪音（非信息） 数据 = 噪音 + 信息 信源：产生信息的实体 信源符号 $s_i$ 发生概率 $p_i$ 自信息：$I(s_i)=-\log p_i$ 信息熵：$H(S)=\sum_{i=1}^np_iI(s_i)$ 信源发出符号平均信息量，衡量不确定度 编码的最优策略 二为底：bit e 为底：纳特 条件自信息：$I(x_i|y_j)=-\log p(x_i|y_i)$ 条件熵：$H(X|Y)=E[I(x_i|y_j)]=-\sum_{i=1}^m\sum_{j=1}^np(x_i,y_j)\log p(x_i|y_j)$ 互信息：$I(X|Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)$ 接受到一个变量使另一个变量不确定度减少的量 联合熵：$H(X,Y)=-\sum_{i=1}^m\sum_{j=1}^np(x_i,y_j)\log p(x_i,y_j)$ 交叉熵：$H(P,Q)=-\sum_{i}p_i\log q_i$ $P$ 基于 $Q$ 编码时平均比特数 相对熵（KL 散度，信息增益）：$D_{\text{KL}}(P||Q)=-\sum_i p_i\ln\frac{q_i}{p_i}$ 使用基于 $Q$ 的分布来编码服从 $P$ 的分布的样本所需的额外的平均比特数 $H(p,q) = H(p) + D_{\text{KL}}(P||Q)$ </description>
    </item>
    
  </channel>
</rss>
